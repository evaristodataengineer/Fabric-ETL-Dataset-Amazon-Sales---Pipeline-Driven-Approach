# ğŸ“Š Pipeline ETL de Datos de Ventas en Microsoft Fabric

> **Proyecto completo de ingenierÃ­a de datos** que demuestra el dominio de Microsoft Fabric para la construcciÃ³n de pipelines ETL empresariales, procesamiento de datos con PySpark y creaciÃ³n de flujos de datos automatizados.

[![Microsoft Fabric](https://img.shields.io/badge/Microsoft%20Fabric-Enabled-blue?style=flat&logo=microsoft)](https://fabric.microsoft.com/)
[![PySpark](https://img.shields.io/badge/PySpark-3.x-orange?style=flat&logo=apache-spark)](https://spark.apache.org/)
[![Python](https://img.shields.io/badge/Python-3.x-blue?style=flat&logo=python)](https://www.python.org/)

---

## ğŸ¯ DescripciÃ³n del Proyecto

Este proyecto implementa una **soluciÃ³n ETL completa en Microsoft Fabric** para el procesamiento y anÃ¡lisis de datos de ventas. El pipeline extrae datos desde fuentes externas (Amazon Sales), los transforma aplicando lÃ³gica de negocio compleja utilizando PySpark, y los carga en un lakehouse optimizado para anÃ¡lisis empresarial.

### ğŸŒŸ CaracterÃ­sticas Principales

- **Pipeline ETL Automatizado**: OrquestaciÃ³n completa del flujo de datos desde la extracciÃ³n hasta la carga
- **Procesamiento Distribuido**: Transformaciones de datos usando PySpark en notebooks de Fabric
- **Lakehouse Medallion Architecture**: ImplementaciÃ³n de capas Bronze, Silver y Gold para organizaciÃ³n de datos
- **Data Flow Integration**: IntegraciÃ³n de flujos de datos con mÃºltiples conectores y transformaciones
- **Deployment Pipelines**: Canalizaciones de implementaciÃ³n para facilitar el movimiento de soluciones entre entornos

---

## ğŸ—ï¸ Arquitectura del Proyecto

```mermaid
flowchart LR
    A[Fuente de Datos<br/>Amazon Sales] --> B[Copy Activity<br/>Fabric Pipeline]
    B --> C[Bronze Layer<br/>Lakehouse]
    C --> D[TransformaciÃ³n<br/>PySpark Notebooks]
    D --> E[Silver Layer<br/>Datos Procesados]
    E --> F[Gold Layer<br/>Datos AnalÃ­ticos]
    F --> G[AnÃ¡lisis y<br/>VisualizaciÃ³n]
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#cd7f32,stroke:#333,stroke-width:2px
    style E fill:#c0c0c0,stroke:#333,stroke-width:2px
    style F fill:#ffd700,stroke:#333,stroke-width:2px
    style G fill:#90EE90,stroke:#333,stroke-width:2px
```

---

## ğŸ”§ Componentes TÃ©cnicos

### 1ï¸âƒ£ **Data Pipeline - OrquestaciÃ³n ETL**

El pipeline principal coordina todo el proceso de ingesta y transformaciÃ³n de datos:

- **Copy Data Activity**: ExtracciÃ³n de datos desde fuentes externas con configuraciÃ³n de:
  - OptimizaciÃ³n inteligente de rendimiento (automÃ¡tico)
  - Paralelismo de copia automÃ¡tico
  - Tolerancia a errores configurable
  - HabilitaciÃ³n de registro de actividades

- **Notebook Activities**: EjecuciÃ³n secuencial de transformaciones PySpark
- **Control Flow**: LÃ³gica de control para gestiÃ³n de errores y excepciones

### 2ï¸âƒ£ **Notebooks PySpark - TransformaciÃ³n de Datos**

#### ğŸ““ **Notebook 1: Load Data (Carga Inicial)**
```python
# Carga de datos brutos desde archivos CSV
df = spark.read.format("csv") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .load("Files/ventas_amazon/")

# ExploraciÃ³n inicial de datos
df.printSchema()
df.show()
```

**Operaciones realizadas:**
- Lectura de archivos CSV con inferencia de esquema
- ValidaciÃ³n de calidad de datos
- Carga inicial en formato Delta para optimizaciÃ³n

#### ğŸ““ **Notebook 2: Sales Aggregation (Agregaciones de Ventas)**
```python
# Agregaciones por categorÃ­a de producto
df_product_revenue = df.groupBy("Category", "Year_month") \
    .agg(
        sum("product_selling_price").alias("total_revenue"),
        count("product_id").alias("total_products"),
        avg("rating").alias("avg_rating")
    )

# CÃ¡lculos de mÃ©tricas de negocio
df_profit_metrics = df.withColumn(
    "profit_margin",
    (col("actual_price") - col("discounted_price")) / col("actual_price")
)
```

**Capacidades demostradas:**
- Agregaciones complejas con mÃºltiples funciones
- CreaciÃ³n de mÃ©tricas de negocio calculadas
- Transformaciones de columnas con Window Functions
- OptimizaciÃ³n de queries con broadcast joins

#### ğŸ““ **Notebook 3: Customer Segmentation (SegmentaciÃ³n de Clientes)**
```python
# SegmentaciÃ³n de clientes por comportamiento de compra
from pyspark.sql.window import Window

window_spec = Window.partitionBy("user_id").orderBy("rating_count")

df_customer_segments = df.withColumn(
    "customer_category",
    when(col("rating_count") > 1000, "High_Engagement")
    .when(col("rating_count") > 500, "Medium_Engagement")
    .otherwise("Low_Engagement")
)
```

**AnÃ¡lisis implementados:**
- SegmentaciÃ³n de clientes basada en mÃ©tricas de engagement
- AnÃ¡lisis de valor de cliente (CLV)
- IdentificaciÃ³n de productos best-sellers
- AnÃ¡lisis temporal de tendencias de ventas

### 3ï¸âƒ£ **Lakehouse - Almacenamiento Delta Lake**

ImplementaciÃ³n de arquitectura medallion:

- **ğŸ¥‰ Bronze Layer**: Datos brutos sin procesar (copia exacta de origen)
- **ğŸ¥ˆ Silver Layer**: Datos limpios y validados con esquema definido
- **ğŸ¥‡ Gold Layer**: Datos agregados listos para consumo analÃ­tico

**Ventajas del enfoque:**
- Trazabilidad completa de transformaciones
- RecuperaciÃ³n ante fallos (time travel)
- OptimizaciÃ³n de consultas con formato Delta
- Particionamiento inteligente de datos

### 4ï¸âƒ£ **Data Flow - Flujo de Datos Visual**

CreaciÃ³n de flujos de datos sin cÃ³digo con:
- Conectores a mÃºltiples orÃ­genes de datos
- Transformaciones visuales (filtros, agregaciones, joins)
- Mapeo de columnas y transformaciones de tipos de datos
- Carga incremental optimizada

### 5ï¸âƒ£ **Deployment Pipelines - Canalizaciones de ImplementaciÃ³n** ğŸš€

> **CaracterÃ­stica destacada del proyecto**: ImplementaciÃ³n de DevOps para Microsoft Fabric

La **canalizaciÃ³n de implementaciÃ³n** (Deployment Pipeline) configurada en este proyecto permite:

#### âœ¨ Â¿QuÃ© son las Deployment Pipelines?

Las Deployment Pipelines en Microsoft Fabric son una caracterÃ­stica de **CI/CD (Continuous Integration/Continuous Deployment)** que facilita el ciclo de vida del desarrollo de soluciones de datos empresariales.

#### ğŸ¯ Beneficios Implementados

1. **SeparaciÃ³n de Entornos**:
   - **Development (Desarrollo)**: Espacio de trabajo para pruebas y desarrollo
   - **Test (Pruebas)**: ValidaciÃ³n de cambios antes de producciÃ³n
   - **Production (ProducciÃ³n)**: Entorno en vivo para usuarios finales

2. **MigraciÃ³n Simplificada de Artefactos**:
   - Movimiento de pipelines, notebooks, lakehouse y datasets entre entornos
   - Un solo clic para desplegar cambios validados
   - Rollback rÃ¡pido en caso de problemas

3. **Control de Versiones y Gobernanza**:
   - Historial de implementaciones
   - Aprobaciones antes de mover a producciÃ³n
   - Trazabilidad de cambios en objetos de Fabric

4. **ConfiguraciÃ³n de ParÃ¡metros por Entorno**:
   - Diferentes cadenas de conexiÃ³n segÃºn el entorno
   - ConfiguraciÃ³n de variables especÃ­ficas
   - Seguridad y aislamiento de datos

#### ğŸ”„ Flujo de Trabajo Implementado

```mermaid
flowchart LR
    A[Desarrollo<br/>Development] -->|Validar y Probar| B[Pruebas<br/>Test]
    B -->|AprobaciÃ³n| C[ProducciÃ³n<br/>Production]
    C -->|Rollback si es necesario| B
    
    style A fill:#87CEEB,stroke:#333,stroke-width:2px
    style B fill:#FFD700,stroke:#333,stroke-width:2px
    style C fill:#90EE90,stroke:#333,stroke-width:2px
```

**Proceso de implementaciÃ³n:**
1. Desarrollo de pipelines y notebooks en el workspace de Development
2. ImplementaciÃ³n a Test para validaciÃ³n de QA
3. EjecuciÃ³n de pruebas de integraciÃ³n y rendimiento
4. Despliegue final a Production tras aprobaciÃ³n
5. Monitoreo continuo y posibilidad de rollback

Esta prÃ¡ctica demuestra **conocimientos avanzados en DevOps para Data Engineering**, una habilidad muy valorada en entornos empresariales modernos.

---

## ğŸ’¼ Habilidades Demostradas

Este proyecto evidencia competencias tÃ©cnicas en:

### ğŸ“ Microsoft Fabric
- âœ… CreaciÃ³n y gestiÃ³n de **Workspaces**
- âœ… ConfiguraciÃ³n de **Lakehouses** con Delta Lake
- âœ… Desarrollo de **Data Pipelines** con copy activities y control flow
- âœ… ImplementaciÃ³n de **Notebooks PySpark**
- âœ… ConfiguraciÃ³n de **Deployment Pipelines** para CI/CD
- âœ… IntegraciÃ³n de **Data Flows** para transformaciones visuales

### ğŸ PySpark y Procesamiento Distribuido
- âœ… Lectura y escritura de datos en formato Delta
- âœ… Transformaciones complejas con DataFrames API
- âœ… Agregaciones y funciones de ventana (Window Functions)
- âœ… OptimizaciÃ³n de queries con broadcast y particionamiento
- âœ… Operaciones de limpieza y validaciÃ³n de datos

### ğŸ“Š IngenierÃ­a de Datos
- âœ… DiseÃ±o de arquitectura medallion (Bronze, Silver, Gold)
- âœ… ImplementaciÃ³n de pipelines ETL escalables
- âœ… Modelado de datos para anÃ¡lisis empresarial
- âœ… GestiÃ³n de calidad de datos y validaciones
- âœ… DevOps para Data Engineering con deployment pipelines

### âš™ï¸ Buenas PrÃ¡cticas
- âœ… CÃ³digo modular y reutilizable
- âœ… DocumentaciÃ³n de transformaciones
- âœ… Versionado de esquemas
- âœ… SeparaciÃ³n de entornos (Dev/Test/Prod)
- âœ… Logging y monitoreo de pipelines

---

## ğŸ“¸ Capturas del Proyecto

El proyecto incluye evidencia visual del proceso completo:

- **ConfiguraciÃ³n de Copy Activity** con optimizaciÃ³n automÃ¡tica de rendimiento
- **Pipeline completo** con mÃºltiples aktividades encadenadas
- **Notebooks PySpark** con transformaciones de datos complejas
- **Lakehouse structure** organizado por capas
- **Data Flow** con transformaciones visuales
- **Deployment Pipeline** configurado para migraciÃ³n entre entornos
- **Resultados de ejecuciÃ³n** exitosa de todas las actividades

---

## ğŸš€ Flujo de EjecuciÃ³n

1. **ExtracciÃ³n**: El pipeline ejecuta la Copy Activity para traer datos desde la fuente Amazon Sales
2. **Carga Bronze**: Los datos brutos se almacenan en el Lakehouse (capa Bronze)
3. **TransformaciÃ³n Silver**: Notebook 1 carga y valida los datos, generando la capa Silver
4. **Agregaciones**: Notebook 2 realiza cÃ¡lculos de revenue y mÃ©tricas de producto
5. **SegmentaciÃ³n**: Notebook 3 genera segmentos de clientes y anÃ¡lisis de comportamiento
6. **Carga Gold**: Los datos transformados se guardan en la capa Gold para consumo analÃ­tico
7. **Deployment**: Los artefactos se pueden mover a Test/Production mediante la deployment pipeline

---

## ğŸ¯ Casos de Uso Empresarial

Este proyecto puede adaptarse a mÃºltiples escenarios empresariales:

- ğŸ“¦ **E-commerce**: AnÃ¡lisis de ventas y comportamiento de clientes
- ğŸª **Retail**: OptimizaciÃ³n de inventario basado en tendencias
- ğŸ“ˆ **Business Intelligence**: Dashboards ejecutivos con mÃ©tricas en tiempo real
- ğŸ” **Data Science**: PreparaciÃ³n de datos para modelos de ML
- ğŸŒ **Multi-tenant SaaS**: SeparaciÃ³n de datos por cliente con deployment pipelines

---

## ğŸ“‹ Requisitos TÃ©cnicos

- Microsoft Fabric Workspace (capacidad F2 o superior recomendada)
- Licencia de Microsoft Fabric o Power BI Premium
- Conocimientos de:
  - PySpark y Python
  - SQL para queries sobre Delta Lake
  - Conceptos de ETL y Data Warehousing

---

## ğŸ“ Aprendizajes y Conclusiones

### Fortalezas del Proyecto
- âœ… Arquitectura escalable y mantenible
- âœ… SeparaciÃ³n clara de responsabilidades por capas
- âœ… ReutilizaciÃ³n de cÃ³digo mediante notebooks modulares
- âœ… ImplementaciÃ³n de CI/CD con deployment pipelines

### Mejoras Futuras
- ğŸ”„ Implementar pruebas unitarias para notebooks PySpark
- ğŸ“Š Agregar dashboards de Power BI conectados al Gold layer
- âš¡ Incorporar streaming de datos en tiempo real
- ğŸ” AÃ±adir controles de seguridad a nivel de fila (RLS)
- ğŸ¤– Integrar modelos de Machine Learning para predicciones

---

## ğŸ‘¤ Sobre el Autor

Este proyecto fue desarrollado como demostraciÃ³n de competencias en **Microsoft Fabric** y **Data Engineering** para aplicaciones empresariales modernas. Refleja la capacidad de diseÃ±ar, implementar y gestionar soluciones de datos end-to-end en entornos cloud.

### Habilidades Clave
- ğŸ’» **Plataformas**: Microsoft Fabric, Azure, Databricks
- ğŸ **Lenguajes**: Python, PySpark, SQL
- ğŸ“Š **Herramientas**: Power BI, Delta Lake, Apache Spark
- ğŸ”§ **MetodologÃ­as**: Data Engineering, ETL, Medallion Architecture, DevOps

---

## ğŸ“ Contacto

Â¿Interesado en discutir soluciones de datos o colaborar en proyectos?

- ğŸ’¼ LinkedIn: [Tu perfil de LinkedIn]
- ğŸ“§ Email: [Tu email profesional]
- ğŸŒ Portfolio: [Tu sitio web]

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ disponible como portfolio personal. El cÃ³digo y la arquitectura pueden ser utilizados como referencia para proyectos similares.

---

<div align="center">

**â­ Si este proyecto te resultÃ³ Ãºtil, considera darle una estrella en GitHub â­**

*Desarrollado con â¤ï¸ utilizando Microsoft Fabric*

</div>
