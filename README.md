# üìä Pipeline ETL de Datos de Ventas en Microsoft Fabric

> **Proyecto completo de ingenier√≠a de datos** que demuestra el dominio de Microsoft Fabric para la construcci√≥n de pipelines ETL empresariales, procesamiento de datos con PySpark y creaci√≥n de flujos de datos automatizados.

[![Microsoft Fabric](https://img.shields.io/badge/Microsoft%20Fabric-Enabled-blue?style=flat&logo=microsoft)](https://fabric.microsoft.com/)
[![PySpark](https://img.shields.io/badge/PySpark-3.x-orange?style=flat&logo=apache-spark)](https://spark.apache.org/)
[![Python](https://img.shields.io/badge/Python-3.x-blue?style=flat&logo=python)](https://www.python.org/)

---

## üìë Tabla de Contenidos

1. [Descripci√≥n del Proyecto](#-descripci√≥n-del-proyecto)
2. [Arquitectura del Proyecto](#-arquitectura-del-proyecto)
3. [Proceso Paso a Paso](#-proceso-paso-a-paso)
   - [Paso 1: Configuraci√≥n del Copy Activity](#paso-1-configuraci√≥n-del-copy-activity)
   - [Paso 2: Ejecuci√≥n del Pipeline de Ingesta](#paso-2-ejecuci√≥n-del-pipeline-de-ingesta)
   - [Paso 3: Carga de Datos en el Notebook](#paso-3-carga-de-datos-en-el-notebook)
   - [Paso 4: Transformaci√≥n y Limpieza de Datos](#paso-4-transformaci√≥n-y-limpieza-de-datos)
   - [Paso 5: Pipeline Completo con Validaciones](#paso-5-pipeline-completo-con-validaciones)
   - [Paso 6: An√°lisis de Calidad de Datos](#paso-6-an√°lisis-de-calidad-de-datos)
   - [Paso 7: Transformaciones Avanzadas](#paso-7-transformaciones-avanzadas)
   - [Paso 8: Agregaciones y M√©tricas de Negocio](#paso-8-agregaciones-y-m√©tricas-de-negocio)
   - [Paso 9: Creaci√≥n de Tablas Dimensionales](#paso-9-creaci√≥n-de-tablas-dimensionales)
   - [Paso 10: Reporte de Calidad de Datos](#paso-10-reporte-de-calidad-de-datos)
   - [Paso 11: Creaci√≥n de Capa Gold - Agregaciones](#paso-11-creaci√≥n-de-capa-gold---agregaciones)
   - [Paso 12: Top Productos y An√°lisis](#paso-12-top-productos-y-an√°lisis)
   - [Paso 13: An√°lisis de Descuentos](#paso-13-an√°lisis-de-descuentos)
   - [Paso 14: Customer Insights](#paso-14-customer-insights)
   - [Paso 15: Executive Summary](#paso-15-executive-summary)
   - [Paso 16: Conexi√≥n de Origen de Datos](#paso-16-conexi√≥n-de-origen-de-datos)
   - [Paso 17: Pipeline Master con Programaci√≥n](#paso-17-pipeline-master-con-programaci√≥n)
   - [Paso 18: Deployment Pipeline - CI/CD](#paso-18-deployment-pipeline---cicd)
4. [Habilidades Demostradas](#-habilidades-demostradas)
5. [Tecnolog√≠as Utilizadas](#-tecnolog√≠as-utilizadas)
6. [Contacto](#-contacto)

---

## üéØ Descripci√≥n del Proyecto

Este proyecto implementa una **soluci√≥n ETL completa en Microsoft Fabric** para el procesamiento y an√°lisis de datos de ventas de Amazon. El pipeline extrae datos desde fuentes externas, los transforma aplicando l√≥gica de negocio compleja utilizando PySpark, y los carga en un lakehouse optimizado para an√°lisis empresarial.

### üåü Caracter√≠sticas Principales

- **Pipeline ETL Automatizado**: Orquestaci√≥n completa del flujo de datos desde la extracci√≥n hasta la carga
- **Procesamiento Distribuido**: Transformaciones de datos usando PySpark en notebooks de Fabric
- **Lakehouse Medallion Architecture**: Implementaci√≥n de capas Bronze, Silver y Gold para organizaci√≥n de datos
- **Data Flow Integration**: Integraci√≥n de flujos de datos con m√∫ltiples conectores y transformaciones
- **Deployment Pipelines**: Canalizaciones de implementaci√≥n para facilitar el movimiento de soluciones entre entornos (Development ‚Üí Test ‚Üí Production)
- **Programaci√≥n Automatizada**: Ejecuci√≥n programada de pipelines para actualizaci√≥n continua de datos

---

## üèóÔ∏è Arquitectura del Proyecto

```mermaid
flowchart TB
    subgraph Fuentes["üåê Fuentes de Datos"]
        A[Amazon Sales Data<br/>CSV Files]
    end
    
    subgraph Ingesta["üì• Ingesta"]
        B[Copy Activity<br/>Fabric Pipeline]
    end
    
    subgraph Bronze["ü•â Bronze Layer"]
        C[Lakehouse<br/>Datos Brutos]
    end
    
    subgraph Transformacion["‚öôÔ∏è Transformaci√≥n - PySpark Notebooks"]
        D1[Notebook 1<br/>Load Data]
        D2[Notebook 2<br/>Sales Aggregation]
        D3[Notebook 3<br/>Customer Insights]
    end
    
    subgraph Silver["ü•à Silver Layer"]
        E[Datos Limpios<br/>y Validados]
    end
    
    subgraph Gold["ü•á Gold Layer"]
        F1[Category Metrics]
        F2[Top Products]
        F3[Discount Analysis]
        F4[Customer Insights]
        F5[Executive Summary]
    end
    
    subgraph Consumo["üìä Consumo"]
        G[Power BI<br/>Dashboards]
    end
    
    A --> B
    B --> C
    C --> D1
    D1 --> D2
    D2 --> D3
    D1 --> E
    D2 --> E
    D3 --> E
    E --> F1
    E --> F2
    E --> F3
    E --> F4
    E --> F5
    F1 --> G
    F2 --> G
    F3 --> G
    F4 --> G
    F5 --> G
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#cd7f32,stroke:#333,stroke-width:2px
    style E fill:#c0c0c0,stroke:#333,stroke-width:2px
    style F1 fill:#ffd700,stroke:#333,stroke-width:2px
    style F2 fill:#ffd700,stroke:#333,stroke-width:2px
    style F3 fill:#ffd700,stroke:#333,stroke-width:2px
    style F4 fill:#ffd700,stroke:#333,stroke-width:2px
    style F5 fill:#ffd700,stroke:#333,stroke-width:2px
    style G fill:#90EE90,stroke:#333,stroke-width:2px
```

---

## üîß Proceso Paso a Paso

A continuaci√≥n se documenta todo el proceso de construcci√≥n del pipeline ETL, desde la configuraci√≥n inicial hasta la implementaci√≥n en producci√≥n.

---

### Paso 1: Configuraci√≥n del Copy Activity

El primer paso consiste en configurar la actividad de copia de datos (Copy Activity) que extrae los datos desde la fuente externa hacia nuestro Lakehouse.

![Configuraci√≥n del Copy Activity - Copiar datos desde CSV a Bronze Layer](0.PNG)

**¬øQu√© estamos haciendo aqu√≠?**

En esta captura se muestra la configuraci√≥n de la actividad **"Copy_CVS_to_Bronze"** dentro del pipeline de datos. Esta actividad es fundamental ya que es el punto de entrada de todos los datos al sistema.

**Configuraciones aplicadas:**

| Configuraci√≥n | Valor | Descripci√≥n |
|---------------|-------|-------------|
| Optimizaci√≥n inteligente del rendimiento | Autom√°tico | Fabric optimiza autom√°ticamente el rendimiento de la copia |
| Grado de paralelismo de la copia | Autom√°tico | Se ajusta din√°micamente seg√∫n la carga |
| Comprobaci√≥n de coherencia de datos | Desactivada | Para mejor rendimiento en carga inicial |
| Tolerancia a errores | Omitir las filas incompatibles | Contin√∫a la carga aunque haya errores menores |
| Habilitar el registro | ‚úÖ Activado | Registra toda la actividad para auditor√≠a |

**¬øPor qu√© es importante esta configuraci√≥n?**

- La **tolerancia a errores** configurada para omitir filas incompatibles nos permite cargar datos aunque algunos registros tengan problemas de formato
- El **registro habilitado** nos proporciona trazabilidad completa de la operaci√≥n
- La **optimizaci√≥n autom√°tica** garantiza el mejor rendimiento sin necesidad de ajustes manuales

---

### Paso 2: Ejecuci√≥n del Pipeline de Ingesta

Una vez configurado, ejecutamos el pipeline para verificar que la ingesta funciona correctamente.

![Ejecuci√≥n exitosa del Pipeline de Ingesta](1.PNG)

**¬øQu√© observamos en esta captura?**

Esta imagen muestra el resultado de la ejecuci√≥n del pipeline de ingesta con **estado exitoso**. Podemos ver:

- **Nombre del pipeline**: `PL_Ingest_amazon_Sales_Bronze`
- **Actividades ejecutadas**: 
  - `Copy_CVS_to_Bron` (Copiar CSV a Bronze)
  - `Copy_CVS_to_Bron_particion` (Copiar CSV con particionamiento)
- **Estado de la canalizaci√≥n**: ‚úÖ Correcto
- **Duraci√≥n**: Tiempo de ejecuci√≥n registrado para cada actividad

**Validaci√≥n de la salida:**

El panel derecho muestra "Salida de validaci√≥n de Canalizaci√≥n" confirmando que:
- Se valid√≥ la canalizaci√≥n correctamente
- No se encontraron errores cr√≠ticos
- Los datos fueron transferidos al destino

---

### Paso 3: Carga de Datos en el Notebook

Con los datos ya en el Lakehouse (capa Bronze), procedemos a cargarlos en un notebook PySpark para su procesamiento.

![Notebook de carga de datos - Configuraci√≥n inicial](2.PNG)

**¬øQu√© estamos haciendo aqu√≠?**

En este notebook estamos configurando la conexi√≥n al Lakehouse y definiendo las rutas de los archivos de datos. El c√≥digo visible muestra:

```python
from pyspark.sql.functions import *
from pyspark.sql import SparkSession

# Definici√≥n de rutas al Lakehouse
amazon_path = "Files/amazon/amazon_sales/amazon_data4471.csv"
storage_path = "Files/amazon/production_amazon_data4471.csv"
```

**Componentes del Explorador (panel izquierdo):**

Se puede observar la estructura del Lakehouse:
- üìÅ **amazon_sales_enterprise_pl**: Lakehouse principal
- üìÅ **Files**: Carpeta de archivos
  - üìÅ **amazon_sales**: Datos de ventas
  - üìÅ **WebDatasets**: Datasets externos
- üìÅ **Tables**: Tablas Delta Lake creadas

Esta estructura sigue las mejores pr√°cticas de organizaci√≥n de datos en Fabric.

---

### Paso 4: Transformaci√≥n y Limpieza de Datos

Continuamos con la transformaci√≥n y limpieza de los datos cargados.

![Transformaci√≥n de datos con PySpark](3.PNG)

**¬øQu√© estamos haciendo aqu√≠?**

Esta captura muestra c√≥digo PySpark avanzado para la transformaci√≥n de datos:

```python
# Configuraci√≥n de la sesi√≥n Spark
df_bronze = spark.read.format("csv") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .load(amazon_path)

# Visualizaci√≥n del esquema de datos
df_bronze.printSchema()
```

**Salida del esquema (panel inferior):**

El notebook muestra la estructura de los datos con todos los campos detectados:
- `product_id`: Identificador del producto
- `product_name`: Nombre del producto
- `category`: Categor√≠a del producto
- `discounted_price`: Precio con descuento
- `actual_price`: Precio original
- `discount_percentage`: Porcentaje de descuento
- `rating`: Calificaci√≥n del producto
- `rating_count`: N√∫mero de calificaciones
- Y m√°s campos relacionados con ventas...

**Panel de Copilot (derecha):**

Se observa la integraci√≥n con **Microsoft Copilot** que proporciona sugerencias inteligentes para el c√≥digo PySpark.

---

### Paso 5: Pipeline Completo con Validaciones

Construimos el pipeline completo que incluye actividades de validaci√≥n y bifurcaci√≥n condicional.

![Pipeline completo con Control Flow](5.PNG)

**¬øQu√© estamos haciendo aqu√≠?**

Esta captura muestra un **pipeline orquestado** con m√∫ltiples componentes:

**Flujo del Pipeline:**

```
Copy Data ‚Üí Data Flow ‚Üí If Condition ‚Üí Notebook (Bronze_Metadata)
                                    ‚Üí If True ‚Üí Copy_CVS_to_Bron_particion
```

**Componentes visibles:**

1. **Copy Data Activity (Copiar datos)**: Ingesta inicial de datos
2. **Data Flow (Blob de datos)**: Transformaci√≥n visual de datos
3. **If Condition (Condici√≥n If)**: L√≥gica condicional para bifurcaci√≥n
4. **Notebook Activity**: Ejecuci√≥n de transformaciones PySpark

**Generador de expresiones de canalizaci√≥n (panel derecha):**

Se muestra la configuraci√≥n de expresiones din√°micas para:
- Evaluar resultados de actividades anteriores
- Controlar el flujo basado en metadatos
- Pasar par√°metros entre actividades

**Expresi√≥n visible:**
```
@greater(activity('Add_Bron_Metadata').output.result_attributes, 0)
```

Esta expresi√≥n verifica si la actividad de metadatos produjo resultados antes de continuar.

---

### Paso 6: An√°lisis de Calidad de Datos

Realizamos un an√°lisis profundo de la calidad de los datos para identificar problemas.

![An√°lisis de Calidad de Datos - Schema y Validaci√≥n](6.PNG)

**¬øQu√© estamos haciendo aqu√≠?**

Este notebook muestra el an√°lisis del esquema de datos y la identificaci√≥n de tipos de datos:

```python
# Visualizar transformaci√≥n de datos
df_silver = df_bronze.select(
    "product_id",
    "product_name", 
    "category",
    col("discounted_price").cast("string"),
    col("actual_price").cast("string"),
    col("discount_percentage").cast("string"),
    col("rating").cast("string"),
    col("rating_count").cast("string"),
    # ... m√°s campos
)

# Mostrar esquema resultante
df_silver.printSchema()
```

**Esquema resultante (visible en la salida):**

```
root
 |-- product_id: string (nullable = true)
 |-- product_name: string (nullable = true)
 |-- category: string (nullable = true)
 |-- discounted_price: string (nullable = true)
 |-- actual_price: string (nullable = true)
 |-- discount_percentage: string (nullable = true)
 |-- rating: float (nullable = true)
 |-- rating_count: string (nullable = true)
 ...
```

**Secci√≥n inferior - An√°lisis de calidad:**

Se muestra c√≥digo para validar la calidad de datos:
- Conteo de valores nulos por columna
- Identificaci√≥n de duplicados
- Validaci√≥n de rangos de valores

---

### Paso 7: Transformaciones Avanzadas

Aplicamos transformaciones avanzadas para limpiar y enriquecer los datos.

![Transformaciones Avanzadas con PySpark](7.PNG)

**¬øQu√© estamos haciendo aqu√≠?**

Esta captura muestra transformaciones complejas de datos:

```python
# Limpieza de campos de precio (eliminar s√≠mbolos de moneda)
df_cleaned = df_silver.withColumn(
    "discounted_price_clean",
    regexp_replace(col("discounted_price"), "[‚Çπ,]", "").cast("float")
).withColumn(
    "actual_price_clean", 
    regexp_replace(col("actual_price"), "[‚Çπ,]", "").cast("float")
)

# C√°lculo del monto de descuento
df_enriched = df_cleaned.withColumn(
    "discount_amount",
    col("actual_price_clean") - col("discounted_price_clean")
)

# Transformaci√≥n de categor√≠as para an√°lisis
df_transformed = df_enriched.withColumn(
    "main_category",
    split(col("category"), "\\|")[0]
).withColumn(
    "sub_category",
    split(col("category"), "\\|")[1]
)
```

**Operaciones realizadas:**

| Operaci√≥n | Descripci√≥n | Prop√≥sito |
|-----------|-------------|-----------|
| `regexp_replace` | Elimina s√≠mbolos de moneda (‚Çπ,) | Convertir texto a num√©rico |
| `cast("float")` | Convierte a tipo flotante | Permitir c√°lculos matem√°ticos |
| `withColumn` | Crea nuevas columnas calculadas | Enriquecer datos |
| `split` | Divide categor√≠as con separador | Crear jerarqu√≠a de categor√≠as |

**Resultado visible en el panel inferior:**

La tabla muestra datos transformados con columnas limpias y calculadas listas para an√°lisis.

---

### Paso 8: Agregaciones y M√©tricas de Negocio

Calculamos m√©tricas de negocio agregadas a nivel de categor√≠a.

![Agregaciones y C√°lculo de M√©tricas](8.PNG)

**¬øQu√© estamos haciendo aqu√≠?**

Este c√≥digo implementa agregaciones complejas para crear m√©tricas de negocio:

```python
# Agregaciones por categor√≠a
from pyspark.sql.functions import sum, avg, count, round

df_category_metrics = df_enriched.groupBy("main_category") \
    .agg(
        count("product_id").alias("total_products"),
        round(avg("rating"), 2).alias("avg_rating"),
        round(sum("discounted_price_clean"), 2).alias("total_revenue"),
        round(avg("discount_percentage_clean"), 2).alias("avg_discount"),
        round(sum("discount_amount"), 2).alias("total_discount_given")
    )

# Ordenar por revenue para identificar categor√≠as top
df_category_metrics = df_category_metrics.orderBy(
    col("total_revenue").desc()
)

# Crear columnas de ranking
from pyspark.sql.window import Window

window_spec = Window.orderBy(col("total_revenue").desc())
df_ranked = df_category_metrics.withColumn(
    "revenue_rank", 
    row_number().over(window_spec)
)
```

**M√©tricas calculadas:**

| M√©trica | Descripci√≥n | Uso de Negocio |
|---------|-------------|----------------|
| `total_products` | Cantidad de productos por categor√≠a | An√°lisis de cat√°logo |
| `avg_rating` | Calificaci√≥n promedio | Calidad percibida |
| `total_revenue` | Ingresos totales | Rendimiento financiero |
| `avg_discount` | Descuento promedio aplicado | Estrategia de precios |
| `total_discount_given` | Total descontado | Impacto en margen |

**Resultado visible:**

Se muestra una tabla con las m√©tricas agregadas por categor√≠a de producto.

---

### Paso 9: Creaci√≥n de Tablas Dimensionales

Creamos las tablas dimensionales para nuestro modelo de datos.

![Creaci√≥n de Tablas Dimensionales - dim_products y fact_reviews](9.PNG)

**¬øQu√© estamos haciendo aqu√≠?**

Este paso crea las tablas dimensionales y de hechos para el modelo anal√≠tico:

```python
# Crear tabla dimensional de productos
print("üì¶ Creando tabla dimensiones: dim_products")

dim_products = df_silver.select(
    "product_id",
    "product_name",
    "category",
    "main_category",
    "sub_category",
    "discounted_price",
    "actual_price",
    "discount_percentage",
    "about_product"
).distinct()

# Persistir en Delta Lake
dim_products.write \
    .mode("overwrite") \
    .format("delta") \
    .saveAsTable("dim_products")

print(f"‚úÖ dim_products creada: {dim_products.count()} productos")
```

**Segunda secci√≥n - Tabla de hechos de reviews:**

```python
# Crear tabla de hechos de reviews
print("‚≠ê Creando tabla de hechos: fact_reviews")

fact_reviews = df_silver.select(
    "product_id",
    "user_id",
    "user_name",
    "review_id",
    "review_title",
    "review_content",
    "rating",
    "rating_count",
    # Agregar columnas de fecha
    current_date().alias("load_date")
)

# Categorizar reviews basado en longitud
fact_reviews = fact_reviews.withColumn(
    "review_length_category",
    when(length(col("review_content")) < 50, "Short")
    .when(length(col("review_content")) < 200, "Medium")
    .otherwise("Long")
)

fact_reviews.write \
    .mode("overwrite") \
    .format("delta") \
    .saveAsTable("fact_reviews")

print(f"‚úÖ fact_reviews creada: {fact_reviews.count()} reviews")
```

**Tablas creadas (visible en panel inferior):**

- ‚úÖ `dim_products` - Dimensi√≥n de productos
- ‚úÖ `fact_reviews` - Hechos de reviews/calificaciones

---

### Paso 10: Reporte de Calidad de Datos

Generamos un reporte completo de la calidad de los datos procesados.

![Reporte de Calidad de Datos Silver Layer](10.PNG)

**¬øQu√© estamos haciendo aqu√≠?**

Este notebook genera un reporte exhaustivo de calidad de datos:

```python
import datetime
from pyspark.sql import SparkSession

print("üìä === REPORTE DE CALIDAD DE DATOS SILVER ===")

# Cargar tablas
fact_reviews = spark.table("fact_reviews")
total_products = dim_products.count()
products_with_reviews = fact_reviews.select("product_id").distinct().count()

# Calcular m√©tricas de calidad
quality_report = {
    "total_records": total_products,
    "records_with_reviews": products_with_reviews,
    "coverage_percentage": (products_with_reviews / total_products) * 100,
    "null_ratings": fact_reviews.filter(col("rating").isNull()).count(),
    "avg_review_length": fact_reviews.agg(avg(length("review_content"))).first()[0]
}

# Mostrar reporte
for metric, value in quality_report.items():
    print(f"   {metric}: {value}")

# Verificar integridad referencial
print("\nüîó Verificaci√≥n de Integridad Referencial:")
print(f"   Productos sin reviews: {total_products - products_with_reviews}")

# Escribir reporte final
print("\n‚úÖ Transformaci√≥n Bronze ‚Üí Silver COMPLETADA EXITOSAMENTE!")
```

**Salida del reporte (visible en panel inferior):**

```
üìä === REPORTE DE CALIDAD DE DATOS SILVER ===
   - Productos en Silver: 488
   - Reviews en Silver: 1,000+
   - Cobertura de reviews: 95.4%
   
‚úÖ Transformaci√≥n Bronze ‚Üí Silver COMPLETADA EXITOSAMENTE!
```

---

### Paso 11: Creaci√≥n de Capa Gold - Agregaciones

Construimos la capa Gold con datos agregados listos para consumo anal√≠tico.

![Creaci√≥n de Capa Gold - Category Metrics](11.PNG)

**¬øQu√© estamos haciendo aqu√≠?**

Este notebook crea las tablas agregadas de la capa Gold:

```python
from pyspark.sql.functions import *

print("ü•á Creando: gold_category_metrics")

# Leer tablas Silver
dim_products = spark.read.table("dim_products")
fact_reviews = spark.read.table("fact_reviews")

# Crear m√©tricas por categor√≠a
gold_category_metrics = dim_products.groupBy("category", "price_category").agg(
    count("product_id").alias("total_products"),
    round(avg("discounted_price"), 2).alias("avg_price"),
    round(avg("discount_percentage"), 2).alias("avg_discount_pct"),
    max("actual_price").alias("max_price"),
    min("discounted_price").alias("min_price")
)

# A√±adir ranking por categor√≠a
window_cat = Window.partitionBy("category").orderBy(col("total_products").desc())

gold_category_metrics = gold_category_metrics.withColumn(
    "rank_in_category",
    dense_rank().over(window_cat)
)

# Guardar en Gold Layer
gold_category_metrics.write \
    .mode("overwrite") \
    .format("delta") \
    .saveAsTable("gold_category_metrics")

print(f"‚úÖ gold_category_metrics creada con {gold_category_metrics.count()} registros")
```

**Resultado visible en el panel inferior:**

Se muestra la tabla `gold_category_metrics` con columnas como:
- `category`: Categor√≠a del producto
- `total_products`: Total de productos en la categor√≠a
- `avg_price`: Precio promedio
- `avg_discount_pct`: Porcentaje de descuento promedio
- `rank_in_category`: Ranking dentro de la categor√≠a

---

### Paso 12: Top Productos y An√°lisis

Identificamos los productos m√°s vendidos y mejor calificados.

![An√°lisis de Top Productos](12.PNG)

**¬øQu√© estamos haciendo aqu√≠?**

Este c√≥digo identifica los productos top por diferentes criterios:

```python
print("üèÜ Creando: gold_top_products")

# Top productos por diferentes criterios
gold_top_products = dim_products.join(
    fact_reviews.groupBy("product_id").agg(
        avg("rating").alias("avg_rating"),
        count("review_id").alias("total_reviews")
    ),
    "product_id"
)

# Ranking por color score (combinaci√≥n de m√©tricas)
window_rating = Window.orderBy(col("avg_rating").desc())
window_reviews = Window.orderBy(col("total_reviews").desc())

gold_top_products = gold_top_products \
    .withColumn("rank_by_rating", dense_rank().over(window_rating)) \
    .withColumn("rank_by_reviews", dense_rank().over(window_reviews))

# Calcular score compuesto
gold_top_products = gold_top_products.withColumn(
    "composite_score",
    (col("avg_rating") * 0.6) + (col("total_reviews") / 100 * 0.4)
)

# Ranking por popularidad (combinaci√≥n rating + reviews)
window_pop = Window.orderBy(col("composite_score").desc())
gold_top_products = gold_top_products.withColumn(
    "popularity_rank",
    dense_rank().over(window_pop)
)

# Filtrar top 100 por cada criterio
gold_top_products_filtered = gold_top_products.filter(
    (col("rank_by_rating") <= 100) | 
    (col("rank_by_reviews") <= 100) |
    (col("popularity_rank") <= 100)
)

gold_top_products_filtered.write \
    .mode("overwrite") \
    .format("delta") \
    .saveAsTable("gold_top_products")

print(f"‚úÖ gold_top_products creada: {gold_top_products_filtered.count()} productos")
```

**Criterios de ranking implementados:**

| Ranking | Criterio | Peso |
|---------|----------|------|
| `rank_by_rating` | Calificaci√≥n promedio | 60% |
| `rank_by_reviews` | Volumen de reviews | 40% |
| `popularity_rank` | Score compuesto | Combinado |

---

### Paso 13: An√°lisis de Descuentos

Analizamos el impacto de los descuentos en las ventas y calificaciones.

![An√°lisis de Descuentos](13.PNG)

**¬øQu√© estamos haciendo aqu√≠?**

Este an√°lisis profundiza en el impacto de la estrategia de descuentos:

```python
print("üí∞ Creando: gold_discount_analysis")

# An√°lisis de descuentos por categor√≠a
gold_discount_analysis = dim_products.groupBy("category").agg(
    count("product_id").alias("total_products"),
    round(avg("discount_percentage"), 2).alias("avg_discount_pct"),
    round(max("discount_percentage"), 2).alias("max_discount_pct"),
    round(sum("discount_amount"), 2).alias("total_discount_amount"),
    # Productos con alto descuento (>30%)
    count(when(col("discount_percentage") > 30, 1)).alias("products_over_30pct_discount"),
    # Productos con descuento moderado (10-30%)
    count(when((col("discount_percentage") >= 10) & (col("discount_percentage") <= 30), 1)).alias("products_10_30pct_discount")
)

# Calcular 5 productos con m√°s descuento
gold_discount_analysis = gold_discount_analysis.withColumn(
    "high_discount_ratio",
    round(col("products_over_30pct_discount") / col("total_products") * 100, 2)
)

gold_discount_analysis.write \
    .mode("overwrite") \
    .format("delta") \
    .saveAsTable("gold_discount_analysis")

print(f"‚úÖ gold_discount_analysis creada")
```

**Resultado visible en el panel inferior:**

Se muestra una tabla con an√°lisis de descuentos incluyendo:

| Categor√≠a | Total Productos | Avg Discount | Max Discount | High Discount % |
|-----------|-----------------|--------------|--------------|-----------------|
| Electronics | 150 | 18.40 | 65 | 12.5% |
| Home & Kitchen | 200 | 22.10 | 70 | 18.2% |
| ... | ... | ... | ... | ... |

---

### Paso 14: Customer Insights

Generamos insights sobre el comportamiento de los clientes.

![Customer Insights - An√°lisis de Clientes](14.PNG)

**¬øQu√© estamos haciendo aqu√≠?**

Este notebook crea un perfil completo de los clientes basado en sus reviews:

```python
print("üë• Creando: gold_customer_insights")

# An√°lisis de comportamiento de clientes
gold_customer_insights = fact_reviews.groupBy("user_id", "user_name").agg(
    count("review_id").alias("total_reviews"),
    round(avg("rating"), 2).alias("avg_rating_given"),
    round(avg(length("review_content")), 0).alias("avg_review_length"),
    countDistinct("product_id").alias("unique_products_reviewed"),
    max("load_date").alias("last_review_date")
)

# Clasificar clientes por engagement
gold_customer_insights = gold_customer_insights.withColumn(
    "customer_segment",
    when(col("total_reviews") >= 10, "Power Reviewer")
    .when(col("total_reviews") >= 5, "Active Reviewer")
    .when(col("total_reviews") >= 2, "Occasional Reviewer")
    .otherwise("New Reviewer")
)

# Identificar si tienden a dar calificaciones altas o bajas
gold_customer_insights = gold_customer_insights.withColumn(
    "rating_tendency",
    when(col("avg_rating_given") >= 4.5, "Generous")
    .when(col("avg_rating_given") >= 3.5, "Balanced")
    .otherwise("Critical")
)

gold_customer_insights.write \
    .mode("overwrite") \
    .format("delta") \
    .saveAsTable("gold_customer_insights")

print(f"‚úÖ gold_customer_insights creada: {gold_customer_insights.count()} usuarios")
```

**Segmentos de clientes creados:**

| Segmento | Criterio | Descripci√≥n |
|----------|----------|-------------|
| Power Reviewer | ‚â•10 reviews | Usuarios muy activos |
| Active Reviewer | 5-9 reviews | Usuarios moderadamente activos |
| Occasional Reviewer | 2-4 reviews | Usuarios ocasionales |
| New Reviewer | 1 review | Usuarios nuevos |

---

### Paso 15: Executive Summary

Creamos un resumen ejecutivo con KPIs clave del negocio.

![Executive Summary - KPIs del Negocio](15.PNG)

**¬øQu√© estamos haciendo aqu√≠?**

Este notebook genera un resumen ejecutivo consolidado:

```python
from datetime import datetime
import pyspark.sql.functions as F

print("üìà Creando: gold_executive_summary")

# Calcular KPIs principales
total_products = dim_products.count()
total_categories = dim_products.select("category").distinct().count()
avg_rating_val = dim_products.agg(avg("rating")).collect()[0][0]
total_revenue = dim_products.agg(sum("discounted_price")).collect()[0][0]

# Crear DataFrame calculado para compartibilidad
summary_data = [
    ("total_products", total_products),
    ("total_categories", total_categories),
    ("avg_product_rating", round(avg_rating_val, 2)),
    ("avg_discount_percent", round(avg_discount, 2)),
    ("total_catalog_value", round(total_revenue, 2)),
    # Productos con descuento significativo
    ("products_discounted_rating", dim_products.filter(col("rating") >= 4.5).count()),
    ("market_coverage", "Amazon India"),
    ("refresh_timestamp", datetime.now().isoformat())
]

# Crear DataFrame de resumen
schema = ["metric_name", "metric_value"]
gold_executive_summary = spark.createDataFrame(summary_data, schema)

gold_executive_summary.write \
    .mode("overwrite") \
    .format("delta") \
    .option("mergeSchema", "true") \
    .saveAsTable("gold_executive_summary")

print("‚úÖ gold_executive_summary creada exitosamente")
```

**KPIs Generados (visible en panel inferior):**

| M√©trica | Valor |
|---------|-------|
| Total Productos | 488 |
| Total Categor√≠as | 15 |
| Rating Promedio | 4.12 |
| Avg Product Price | ‚Çπ1,565.00 |
| ... | ... |

---

### Paso 16: Conexi√≥n de Origen de Datos

Configuramos la conexi√≥n al origen de datos externo para el Data Flow.

![Configuraci√≥n de Conexi√≥n de Origen de Datos](16.PNG)

**¬øQu√© estamos haciendo aqu√≠?**

Esta captura muestra la configuraci√≥n del **conector de origen de datos** en Microsoft Fabric:

**Configuraci√≥n de Credenciales:**

| Campo | Valor |
|-------|-------|
| Conexi√≥n | Canalizaciones de datos de Fabric |
| Nombre de conexi√≥n | evaristo_data_engineer |
| Puerta de enlace de datos | Ninguno (cloud nativo) |
| Tipo de autenticaci√≥n | Cuenta de organizaci√≥n |
| Nivel de privacidad | Ninguno |

**¬øPor qu√© es importante esta configuraci√≥n?**

- **Cuenta de organizaci√≥n**: Permite autenticaci√≥n SSO con Azure AD
- **Sin puerta de enlace**: Indica que los datos residen en la nube, no requiere gateway on-premises
- **Nivel de privacidad**: Configurado para permitir combinaci√≥n de datos entre fuentes

Esta configuraci√≥n es fundamental para que el Data Flow pueda acceder a los datos del Lakehouse y otras fuentes autorizadas.

---

### Paso 17: Pipeline Master con Programaci√≥n

Configuramos el pipeline maestro que orquesta todos los procesos con programaci√≥n autom√°tica.

![Pipeline Master con Programaci√≥n Autom√°tica](17.PNG)

**¬øQu√© estamos haciendo aqu√≠?**

Esta captura muestra el **Pipeline Maestro (PL_MASTER_Amazon_Sales_ETL)** completamente configurado:

**Flujo del Pipeline:**

```mermaid
flowchart LR
    A[Invocar canalizaci√≥n<br/>Ingest_Bron] --> B[Invocar canalizaci√≥n<br/>...] 
    B --> C[Invocar canalizaci√≥n<br/>Ingest_Gold]
    
    style A fill:#87CEEB,stroke:#333,stroke-width:2px
    style B fill:#FFD700,stroke:#333,stroke-width:2px
    style C fill:#90EE90,stroke:#333,stroke-width:2px
```

**Panel de Programaci√≥n (derecha):**

Se ha configurado la **ejecuci√≥n programada** con las siguientes opciones:

| Configuraci√≥n | Valor |
|---------------|-------|
| Programaciones | Todos los d√≠as |
| Hora del d√≠a | Configurada |
| √öltima actualizaci√≥n correcta | Fecha/hora registrada |
| Pr√≥xima actualizaci√≥n | Pr√≥xima ejecuci√≥n programada |
| Estado | ‚úÖ Activar |

**Configuraci√≥n del Pipeline (panel inferior):**

| Par√°metro | Configuraci√≥n |
|-----------|---------------|
| Tipo | Master |
| Canalizaciones | PL_Ingest_amazon_Sales_Bronze |
| Ejecuci√≥n | Activado + Alerta |
| Par√°metros | Variables din√°micas configuradas |

**¬øPor qu√© es importante?**

La programaci√≥n autom√°tica permite:
- Actualizaci√≥n diaria de datos sin intervenci√≥n manual
- Consistencia en la ejecuci√≥n del pipeline
- Alertas en caso de fallos
- Trazabilidad completa de ejecuciones

---

### Paso 18: Deployment Pipeline - CI/CD

**üöÄ Esta es la secci√≥n m√°s importante del proyecto: la implementaci√≥n de DevOps para Microsoft Fabric.**

![Deployment Pipeline - Implementaci√≥n entre Entornos](18.PNG)

**¬øQu√© estamos haciendo aqu√≠?**

Esta captura muestra la configuraci√≥n del **Deployment Pipeline** (Canalizaci√≥n de Implementaci√≥n) que permite mover soluciones de datos entre diferentes entornos de manera controlada y profesional.

**Estructura de Entornos:**

```mermaid
flowchart LR
    subgraph Dev["üîß Development"]
        A1[Lakehouse]
        A2[Notebooks]
        A3[Pipelines]
    end
    
    subgraph Test["üß™ Test"]
        B1[Lakehouse]
        B2[Notebooks]
        B3[Pipelines]
    end
    
    subgraph Prod["üöÄ Production"]
        C1[Lakehouse]
        C2[Notebooks]
        C3[Pipelines]
    end
    
    Dev -->|"Implementar"| Test
    Test -->|"Implementar"| Prod
    
    style Dev fill:#87CEEB,stroke:#333,stroke-width:2px
    style Test fill:#FFD700,stroke:#333,stroke-width:2px
    style Prod fill:#90EE90,stroke:#333,stroke-width:2px
```

**¬øQu√© se observa en la captura?**

1. **Panel "Implementar en esta fase"**: 
   - Muestra el proceso de mover artefactos de Development a Test
   - Lista de elementos a implementar:
     - ‚úÖ `amazon_sales_enterprise_pl` (Lakehouse)
     - ‚úÖ `PL_Ingest_amazon_Sales_Bron` (Pipeline)
     - ‚úÖ `PL_Ingest_amazon_Sales_Silver` (Pipeline)
     - ‚úÖ `PL_Ingest_amazon_Sales_Gold` (Pipeline)
     - ‚úÖ Notebooks asociados

2. **Configuraci√≥n visible:**
   - Selecci√≥n de elementos a desplegar
   - Opci√≥n de "Agregar una Nota" para documentar el cambio
   - Checkbox: "Continuar con la implementaci√≥n en caso de que se produzca... una nueva ubicaci√≥n o con informaci√≥n..."
   - Botones de **Implementar** y **Cancelar**

3. **Panel inferior - Estado de elementos:**
   - Muestra el estado de cada componente en los tres entornos
   - Indica qu√© elementos son nuevos vs actualizados
   - Tracking de versiones por entorno

**¬øPor qu√© son importantes los Deployment Pipelines?**

Los Deployment Pipelines en Microsoft Fabric proporcionan:

| Beneficio | Descripci√≥n |
|-----------|-------------|
| **Separaci√≥n de entornos** | Desarrollo, pruebas y producci√≥n aislados |
| **Control de calidad** | Validaci√≥n obligatoria antes de producci√≥n |
| **Rollback r√°pido** | Capacidad de revertir cambios problem√°ticos |
| **Trazabilidad** | Historial completo de implementaciones |
| **Colaboraci√≥n** | M√∫ltiples desarrolladores sin conflictos |
| **Gobernanza** | Aprobaciones y control de acceso |

**Flujo de trabajo implementado:**

1. **Development**: Los data engineers desarrollan y prueban cambios localmente
2. **Test**: QA valida funcionalidad, rendimiento e integridad de datos
3. **Production**: Solo cambios aprobados llegan a usuarios finales

**Artefactos gestionados:**

- üìÅ Lakehouses y estructura de datos
- üìì Notebooks PySpark
- üîÑ Pipelines de datos
- üìä Data Flows
- üìà Informes y dashboards

**Esta funcionalidad demuestra conocimientos avanzados en:**

- ‚úÖ **DevOps para Data Engineering**
- ‚úÖ **CI/CD en entornos de datos**
- ‚úÖ **Gesti√≥n profesional del ciclo de vida**
- ‚úÖ **Mejores pr√°cticas empresariales**
- ‚úÖ **Gobernanza de datos**

---

## üíº Habilidades Demostradas

Este proyecto evidencia competencias t√©cnicas avanzadas en m√∫ltiples √°reas:

### üéì Microsoft Fabric

| Habilidad | Nivel | Evidencia |
|-----------|-------|-----------|
| Creaci√≥n de Workspaces | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Configuraci√≥n completa de entornos |
| Lakehouses con Delta Lake | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Arquitectura medallion implementada |
| Data Pipelines | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Orquestaci√≥n con control flow |
| Notebooks PySpark | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Transformaciones complejas |
| Deployment Pipelines | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | CI/CD entre entornos |
| Data Flows | ‚≠ê‚≠ê‚≠ê‚≠ê | Transformaciones visuales |
| Programaci√≥n | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Ejecuci√≥n automatizada |

### üêç PySpark y Procesamiento Distribuido

- ‚úÖ Lectura y escritura de datos en formato Delta
- ‚úÖ Transformaciones complejas con DataFrames API
- ‚úÖ Agregaciones y funciones de ventana (Window Functions)
- ‚úÖ Operaciones de limpieza y validaci√≥n de datos
- ‚úÖ Expresiones regulares para transformaci√≥n de texto
- ‚úÖ Joins y combinaci√≥n de datasets
- ‚úÖ Creaci√≥n de m√©tricas calculadas

### üìä Ingenier√≠a de Datos

- ‚úÖ Dise√±o de arquitectura medallion (Bronze ‚Üí Silver ‚Üí Gold)
- ‚úÖ Implementaci√≥n de pipelines ETL escalables
- ‚úÖ Modelado dimensional (dimensiones y hechos)
- ‚úÖ Gesti√≥n de calidad de datos
- ‚úÖ DevOps para Data Engineering
- ‚úÖ Documentaci√≥n t√©cnica profesional

### ‚öôÔ∏è Mejores Pr√°cticas

- ‚úÖ C√≥digo modular y reutilizable
- ‚úÖ Nomenclatura consistente
- ‚úÖ Versionado de esquemas
- ‚úÖ Separaci√≥n de entornos (Dev/Test/Prod)
- ‚úÖ Logging y monitoreo de pipelines
- ‚úÖ Documentaci√≥n de procesos

---

## üõ†Ô∏è Tecnolog√≠as Utilizadas

| Tecnolog√≠a | Uso en el Proyecto |
|------------|-------------------|
| ![Microsoft Fabric](https://img.shields.io/badge/Microsoft%20Fabric-blue?logo=microsoft) | Plataforma principal de datos |
| ![PySpark](https://img.shields.io/badge/PySpark-orange?logo=apache-spark) | Procesamiento de datos distribuido |
| ![Delta Lake](https://img.shields.io/badge/Delta%20Lake-003366?logo=delta) | Formato de almacenamiento |
| ![Python](https://img.shields.io/badge/Python-3.x-blue?logo=python) | Lenguaje de programaci√≥n |

---

## üìä Resumen del Proyecto

| M√©trica | Valor |
|---------|-------|
| **Notebooks desarrollados** | 3+ |
| **Pipelines creados** | 4+ |
| **Tablas Delta Lake** | 8+ (dim + fact + gold) |
| **Capas de datos** | 3 (Bronze, Silver, Gold) |
| **Entornos configurados** | 3 (Dev, Test, Prod) |
| **Ejecuci√≥n** | Programada (diaria) |

---

## üéØ Casos de Uso Empresarial

Este proyecto puede adaptarse a m√∫ltiples escenarios:

- üì¶ **E-commerce**: An√°lisis de ventas y comportamiento de clientes
- üè™ **Retail**: Optimizaci√≥n de inventario y pricing
- üìà **Business Intelligence**: Dashboards ejecutivos
- üîç **Data Science**: Preparaci√≥n de datos para ML
- üåê **Multi-tenant**: Separaci√≥n de datos por cliente

---

## üë§ Sobre el Autor

Este proyecto fue desarrollado como demostraci√≥n de competencias en **Microsoft Fabric** y **Data Engineering** para aplicaciones empresariales modernas.

### √Åreas de Expertise

- üíª **Plataformas**: Microsoft Fabric, Azure, Databricks
- üêç **Lenguajes**: Python, PySpark, SQL
- üìä **Herramientas**: Power BI, Delta Lake, Apache Spark
- üîß **Metodolog√≠as**: ETL, Medallion Architecture, DevOps

---

## üìû Contacto

**Evaristo - Data Engineer**

[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/evaristo-sandoval-gil-86a6a0291/)
[![GitHub](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/evaristodataengineer)


> üíº **En b√∫squeda activa de oportunidades como Data Engineer**

<div align="center">

**‚≠ê Si este proyecto te result√≥ √∫til, considera darle una estrella en GitHub ‚≠ê**

*Desarrollado con ‚ù§Ô∏è utilizando Microsoft Fabric*

</div>

